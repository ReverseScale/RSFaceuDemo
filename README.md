# RSFaceuDemo

![](https://img.shields.io/badge/platform-iOS-red.svg) 
![](https://img.shields.io/badge/language-Objective--C-orange.svg) 
![](https://img.shields.io/badge/download-11.6MB-brightgreen.svg) 
![](https://img.shields.io/badge/license-MIT%20License-brightgreen.svg) 

[EN](#Requirements) | [ä¸­æ–‡](#ä¸­æ–‡è¯´æ˜)

Faceu Face Moe must have had a set of its own core algorithms, so it would say "Someone mimics my face ..."

Recently in the study of some image processing technology, the most common applications are the number of "Faceu Meng", in order to show clearer, I choose to split the way to achieve Demo.

## ğŸ¨ Why test the UI?

| Name | 1. List Page | 2. Filter Effect Page | 3. Green Screen Key Page 1 | 4. Green Screen Key Page 2 | 5. Static Composite Page | 6.Dynamic Synthesis Page |
| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
| æˆªå›¾ | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/98294256.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/68659680.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/33825098.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/25444114.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/17807305.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/28926041.jpg) |
| Description | Build a basic framework via storyboard | Based on the system of several filter effects | Based on GPUImage package to achieve green screen chromakey | Replacing material to achieve green screen keying | Based on GPUImage package to achieve static synthesis | Based on GPUImage package to achieve synthesis |

## ğŸš€ Advantage 
* 1. Less documents, code concise, innovative features
* 2. High functionality integration, good experience, fine structure
* 3. Relies solely on GPUImage to achieve
* 4. Has a high custom

## ğŸ¤– Requirements 
* iOS 7+
* Xcode 8+


## ğŸ›  Usage
### filter effect
Core code display:

```
//Convert UIImage to CIImage
CIImage *ciImage = [[CIImage alloc] initWithImage:self.originImage];
//Create a filter
CIFilter *filter = [CIFilter filterWithName:filterName
                              keysAndValues:kCIInputImageKey,ciImage, nil];
//The existing value does not change, the other is set as the default value
[filter setDefaults];
        
//Get the drawing context
CIContext *context = [CIContext contextWithOptions:nil];
        
//Render and output CIImage
CIImage *outputImage = [filter outputImage];
        
//Create a CGImage handle
CGImageRef cgImage = [context createCGImage:outputImage fromRect:[outputImage extent]];
        
//Get the picture
UIImage *image = [UIImage imageWithCGImage:cgImage];
        
//Release the CGImage handle
CGImageRelease(cgImage);
        
self.imageView.image = image;
```

### green screen keying

Quote header file:
```
#import "RSChromaKeyFilter.h"
```

Tool method:

```
RSChromaKeyFilter *filter=[[RSChromaKeyFilter alloc] initWithInputImage:self.greenImageView.image
                                                        backgroundImage:self.resultBgImageView.image];
    
self.resultImageView.image=[[UIImage imageWithCIImage:filter.outputImage] copy];
```

### Static Synthesis

Quote header file:

```
#import <CoreImage/CoreImage.h>
#import "UIImageView+Gif.h"
```

The core method:

```
for (UIView *view in self.oldImageView.subviews) {
    [view removeFromSuperview];
}

// Image Recognition: You can choose between CIDetectorAccuracyHigh and CIDetectorAccuracyLow, because you want to be more accurate
CIDetectorAccuracyHigh
NSDictionary *opts = [NSDictionary dictionaryWithObject:CIDetectorAccuracyHigh
                                                 forKey:CIDetectorAccuracy];

CIDetector *detector=[CIDetector detectorOfType:CIDetectorTypeFace context:nil options:opts];

//    CIDetector *detector = [CIDetector detectorOfType:CIDetectorTypeFace
//                                              context:nil
//                                              options:nil];

CIImage *image=[[CIImage alloc] initWithImage:self.oldImageView.image];
NSArray *faceArray = [detector featuresInImage:image
                                       options:nil];

/ ** Convert Core Image Coordinates to UIView Coordinates ** /
// get the size of the image
CGSize ciImageSize = [image extent] .size ;;
// Symmetry the image along the y axis
CGAffineTransform transform = CGAffineTransformScale(CGAffineTransformIdentity, 1, -1);
// Negative y-axis translation
transform = CGAffineTransformTranslate(transform,0,-ciImageSize.height);

for (CIFeature *f in faceArray){
    
    if ([f.type isEqualToString:CIFeatureTypeFace]) {
        
        CIFaceFeature *faceFeature=(CIFaceFeature *)f;
        // Achieve coordinate conversion
        CGSize viewSize = self.oldImageView.bounds.size;
        CGFloat scale = MIN(viewSize.width / ciImageSize.width,
                            viewSize.height / ciImageSize.height);
        CGFloat offsetX = (viewSize.width - ciImageSize.width * scale) / 2;
        CGFloat offsetY = (viewSize.height - ciImageSize.height * scale) / 2;
        // Zoom
        CGAffineTransform scaleTransform = CGAffineTransformMakeScale(scale, scale);
        // Get face frame
        CGRect faceViewBounds = CGRectApplyAffineTransform(faceFeature.bounds, transform);
        // Fix
        faceViewBounds = CGRectApplyAffineTransform(faceViewBounds,scaleTransform);
        faceViewBounds.origin.x += offsetX;
        faceViewBounds.origin.y += offsetY;
        
        UIView *faceView=[[UIView alloc] initWithFrame:faceViewBounds];
        faceView.layer.borderWidth=3;
        faceView.layer.borderColor=[UIColor orangeColor].CGColor;
        
        [self.oldImageView addSubview:faceView];
        
        / ** plus light ring ** /
        UIImageView *imageView=[UIImageView new];
        CGFloat haloWidth= faceViewBounds.size.width;
        CGFloat haloHeight= haloWidth * 159 / 351;
        
        CGFloat haloCenterX=faceViewBounds.origin.x+faceViewBounds.size.width/2;
        
        CGRect rect=CGRectMake(haloCenterX-haloWidth/2, faceViewBounds.origin.y-haloHeight, haloWidth, haloHeight);
        imageView.frame=rect;
        [self.oldImageView addSubview:imageView];
        
        
        NSMutableArray *list=[NSMutableArray new];
        for (int i=0; i<41; i++) {
            if (i<10) {
                NSString *name=[NSString stringWithFormat:@"halo_00%d",i];
                UIImage  *image=  [UIImage imageNamed:name];
                [list addObject:image];
            }else{
                NSString *name=[NSString stringWithFormat:@"halo_0%d",i];
                UIImage  *image=  [UIImage imageNamed:name];
                [list addObject:image];
            }
        }
        
        [imageView playGifAnim:[list copy]];
        
        // to determine if there is left eye position
        if(faceFeature.hasLeftEyePosition){
            
            CGFloat x=faceFeature.leftEyePosition.x;
            CGFloat y=faceFeature.leftEyePosition.y;
            CGRect leftEyeRect=CGRectMake(x-10/2,y-10/2, 10, 10);
            
            // Get face frame
            CGRect leftEyeBounds = CGRectApplyAffineTransform(leftEyeRect, transform);
            leftEyeBounds=CGRectApplyAffineTransform(leftEyeBounds,scaleTransform);
            leftEyeBounds.origin.x += offsetX;
            leftEyeBounds.origin.y += offsetY;
            
            UIView *leftEyeView = [[UIView alloc] initWithFrame:leftEyeBounds];
            leftEyeView .backgroundColor = [UIColor orangeColor];
            [self.oldImageView addSubview:leftEyeView ];
            
        }
        // Judge whether there is the right eye position
        if(faceFeature.hasRightEyePosition){
            CGFloat x=faceFeature.rightEyePosition.x;
            CGFloat y=faceFeature.rightEyePosition.y;
            CGRect rightEyeRect=CGRectMake(x-10/2,y-10/2, 10, 10);
            
            //Get face frame
            CGRect rightEyeBounds = CGRectApplyAffineTransform(rightEyeRect, transform);
            rightEyeBounds=CGRectApplyAffineTransform(rightEyeBounds,scaleTransform);
            rightEyeBounds.origin.x += offsetX;
            rightEyeBounds.origin.y += offsetY;
            
            UIView *rightEyeView = [[UIView alloc] initWithFrame:rightEyeBounds];
            rightEyeView.backgroundColor = [UIColor orangeColor];
            [self.oldImageView addSubview:rightEyeView];
        }
        // Judge whether the mouth position
        if(faceFeature.hasMouthPosition){
            
        }
    }
}
```

### Dynamic synthesis

Quote header file:
```
#import <GPUImage/GPUImage.h>
```

The core method:
```
_faceThinking = YES;
CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
CFDictionaryRef attachments = CMCopyDictionaryOfAttachments(kCFAllocatorDefault, sampleBuffer, kCMAttachmentMode_ShouldPropagate);
CIImage *convertedImage = [[CIImage alloc] initWithCVPixelBuffer:pixelBuffer options:(__bridge NSDictionary *)attachments];

if (attachments)
    CFRelease(attachments);
NSDictionary *imageOptions = nil;
UIDeviceOrientation curDeviceOrientation = [[UIDevice currentDevice] orientation];
int exifOrientation;

enum {
    PHOTOS_EXIF_0ROW_TOP_0COL_LEFT			= 1, //   1  =  0th row is at the top, and 0th column is on the left (THE DEFAULT).
    PHOTOS_EXIF_0ROW_TOP_0COL_RIGHT			= 2, //   2  =  0th row is at the top, and 0th column is on the right.
    PHOTOS_EXIF_0ROW_BOTTOM_0COL_RIGHT      = 3, //   3  =  0th row is at the bottom, and 0th column is on the right.
    PHOTOS_EXIF_0ROW_BOTTOM_0COL_LEFT       = 4, //   4  =  0th row is at the bottom, and 0th column is on the left.
    PHOTOS_EXIF_0ROW_LEFT_0COL_TOP          = 5, //   5  =  0th row is on the left, and 0th column is the top.
    PHOTOS_EXIF_0ROW_RIGHT_0COL_TOP         = 6, //   6  =  0th row is on the right, and 0th column is the top.
    PHOTOS_EXIF_0ROW_RIGHT_0COL_BOTTOM      = 7, //   7  =  0th row is on the right, and 0th column is the bottom.
    PHOTOS_EXIF_0ROW_LEFT_0COL_BOTTOM       = 8  //   8  =  0th row is on the left, and 0th column is the bottom.
};
BOOL isUsingFrontFacingCamera = FALSE;
AVCaptureDevicePosition currentCameraPosition = [self.videoCamera cameraPosition];

if (currentCameraPosition != AVCaptureDevicePositionBack) {
    isUsingFrontFacingCamera = TRUE;
}

switch (curDeviceOrientation) {
    case UIDeviceOrientationPortraitUpsideDown:  // Device oriented vertically, home button on the top
        exifOrientation = PHOTOS_EXIF_0ROW_LEFT_0COL_BOTTOM;
        break;
    case UIDeviceOrientationLandscapeLeft:       // Device oriented horizontally, home button on the right
        if (isUsingFrontFacingCamera)
            exifOrientation = PHOTOS_EXIF_0ROW_BOTTOM_0COL_RIGHT;
        else
            exifOrientation = PHOTOS_EXIF_0ROW_TOP_0COL_LEFT;
        break;
    case UIDeviceOrientationLandscapeRight:      // Device oriented horizontally, home button on the left
        if (isUsingFrontFacingCamera)
            exifOrientation = PHOTOS_EXIF_0ROW_TOP_0COL_LEFT;
        else
            exifOrientation = PHOTOS_EXIF_0ROW_BOTTOM_0COL_RIGHT;
        break;
    case UIDeviceOrientationPortrait:            // Device oriented vertically, home button on the bottom
    default:
        exifOrientation = PHOTOS_EXIF_0ROW_RIGHT_0COL_TOP;
        break;
}

imageOptions = [NSDictionary dictionaryWithObject:[NSNumber numberWithInt:exifOrientation] forKey:CIDetectorImageOrientation];
NSArray *features = [self.faceDetector featuresInImage:convertedImage options:imageOptions];

// get the clean aperture
CMFormatDescriptionRef fdesc = CMSampleBufferGetFormatDescription(sampleBuffer);
CGRect clap = CMVideoFormatDescriptionGetCleanAperture(fdesc, false /*originIsTopLeft == false*/);


[self GPUVCWillOutputFeatures:features forClap:clap andOrientation:curDeviceOrientation];
_faceThinking = NO;
```


## âš– License

```
MIT License

Copyright (c) 2017 ReverseScale

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## ğŸ˜¬ Contributions

* WeChat : WhatsXie
* Email : ReverseScale@iCloud.com
* Blog : https://reversescale.github.io

---
# ä¸­æ–‡è¯´æ˜

Faceuè„¸èŒä¸€å®šæ˜¯æœ‰ä¸€å¥—è‡ªå·±çš„æ ¸å¿ƒç®—æ³•ï¼Œæ‰€ä»¥å®ƒä¼šè¯´â€œæœ‰äººæ¨¡ä»¿æˆ‘çš„è„¸...â€

æœ€è¿‘åœ¨ç ”ç©¶ä¸€äº›å›¾åƒå¤„ç†çš„æŠ€æœ¯ï¼Œå…¶ä¸­æœ€å¸¸è§çš„åº”ç”¨å°±è¦æ•° â€œFaceu è„¸èŒâ€ äº†ï¼Œä¸ºäº†å±•ç¤ºæ›´æ¸…æ™°ï¼Œæˆ‘é€‰æ‹©æ‹†åˆ†åŠŸèƒ½çš„æ–¹å¼æ¥å®ç° Demoã€‚

## ğŸ¨ æµ‹è¯• UI ä»€ä¹ˆæ ·å­ï¼Ÿ

| åç§° |1.åˆ—è¡¨é¡µ |2.æ»¤é•œæ•ˆæœé¡µ |3.ç»¿å±æŠ åƒé¡µ1 |4.ç»¿å±æŠ åƒé¡µ2 |5.é™æ€åˆæˆé¡µ |6.åŠ¨æ€åˆæˆé¡µ |
| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
| æˆªå›¾ | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/98294256.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/68659680.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/33825098.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/25444114.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/17807305.jpg) | ![](http://og1yl0w9z.bkt.clouddn.com/17-9-4/28926041.jpg) |
| æè¿° | é€šè¿‡ storyboard æ­å»ºåŸºæœ¬æ¡†æ¶ | åŸºäºç³»ç»Ÿçš„å‡ ç§æ»¤é•œæ•ˆæœ | åŸºäº GPUImage å°è£…å®ç°ç»¿å±æŠ åƒ | æ›´æ¢ç´ æå®ç°ç»¿å±æŠ åƒ | åŸºäº GPUImage å°è£…å®ç°é™æ€åˆæˆ | åŸºäº GPUImage å°è£…å®ç°åˆæˆ |

## ğŸš€ æ¡†æ¶çš„ä¼˜åŠ¿
* 1.æ–‡ä»¶å°‘ï¼Œä»£ç ç®€æ´ï¼ŒåŠŸèƒ½æ–°é¢–
* 2.åŠŸèƒ½æ•´åˆåº¦é«˜ï¼Œä½“éªŒå¥½ï¼Œç»“æ„ç²¾
* 3.å•çº¯ä¾èµ– GPUImage æ¥å®ç°
* 4.å…·å¤‡è¾ƒé«˜è‡ªå®šä¹‰æ€§

## ğŸ¤– è¦æ±‚
* iOS 7+
* Xcode 8+


## ğŸ›  ä½¿ç”¨æ–¹æ³•
### æ»¤é•œæ•ˆæœ 
æ ¸å¿ƒä»£ç å±•ç¤ºï¼š

```
//å°†UIImageè½¬æ¢æˆCIImage
CIImage *ciImage = [[CIImage alloc] initWithImage:self.originImage];
//åˆ›å»ºæ»¤é•œ
CIFilter *filter = [CIFilter filterWithName:filterName
                              keysAndValues:kCIInputImageKey,ciImage, nil];
//å·²æœ‰çš„å€¼ä¸æ”¹å˜ï¼Œå…¶ä»–çš„è®¾ä¸ºé»˜è®¤å€¼
[filter setDefaults];
        
//è·å–ç»˜åˆ¶ä¸Šä¸‹æ–‡
CIContext *context = [CIContext contextWithOptions:nil];
        
//æ¸²æŸ“å¹¶è¾“å‡ºCIImage
CIImage *outputImage = [filter outputImage];
        
//åˆ›å»ºCGImageå¥æŸ„
CGImageRef cgImage = [context createCGImage:outputImage fromRect:[outputImage extent]];
        
//è·å–å›¾ç‰‡
UIImage *image = [UIImage imageWithCGImage:cgImage];
        
//é‡Šæ”¾CGImageå¥æŸ„
CGImageRelease(cgImage);
        
self.imageView.image = image;
```

### ç»¿å±æŠ åƒ

å¼•ç”¨å¤´æ–‡ä»¶ï¼š

```
#import "RSChromaKeyFilter.h"
```

å·¥å…·æ–¹æ³•ï¼š

```
RSChromaKeyFilter *filter=[[RSChromaKeyFilter alloc] initWithInputImage:self.greenImageView.image
                                                        backgroundImage:self.resultBgImageView.image];
    
self.resultImageView.image=[[UIImage imageWithCIImage:filter.outputImage] copy];
```

### é™æ€åˆæˆ

å¼•ç”¨å¤´æ–‡ä»¶ï¼š

```
#import <CoreImage/CoreImage.h>
#import "UIImageView+Gif.h"
```

æ ¸å¿ƒæ–¹æ³•ï¼š

```
for (UIView *view in self.oldImageView.subviews) {
    [view removeFromSuperview];
}

// å›¾åƒè¯†åˆ«èƒ½åŠ›ï¼šå¯ä»¥åœ¨CIDetectorAccuracyHigh(è¾ƒå¼ºçš„å¤„ç†èƒ½åŠ›)ä¸CIDetectorAccuracyLow(è¾ƒå¼±çš„å¤„ç†èƒ½åŠ›)ä¸­é€‰æ‹©ï¼Œå› ä¸ºæƒ³è®©å‡†ç¡®åº¦é«˜ä¸€äº›åœ¨è¿™é‡Œé€‰æ‹©CIDetectorAccuracyHigh
NSDictionary *opts = [NSDictionary dictionaryWithObject:CIDetectorAccuracyHigh
                                                 forKey:CIDetectorAccuracy];

CIDetector *detector=[CIDetector detectorOfType:CIDetectorTypeFace context:nil options:opts];

//    CIDetector *detector = [CIDetector detectorOfType:CIDetectorTypeFace
//                                              context:nil
//                                              options:nil];

CIImage *image=[[CIImage alloc] initWithImage:self.oldImageView.image];
NSArray *faceArray = [detector featuresInImage:image
                                       options:nil];

/** å°† Core Image åæ ‡è½¬æ¢æˆ UIView åæ ‡ **/
//å¾—åˆ°å›¾ç‰‡çš„å°ºå¯¸
CGSize ciImageSize = [image extent].size;;
//å°†imageæ²¿yè½´å¯¹ç§°
CGAffineTransform transform = CGAffineTransformScale(CGAffineTransformIdentity, 1, -1);
//yè½´è´Ÿæ–¹å‘å¹³ç§»
transform = CGAffineTransformTranslate(transform,0,-ciImageSize.height);

for (CIFeature *f in faceArray){
    
    if ([f.type isEqualToString:CIFeatureTypeFace]) {
        
        CIFaceFeature *faceFeature=(CIFaceFeature *)f;
        // å®ç°åæ ‡è½¬æ¢
        CGSize viewSize = self.oldImageView.bounds.size;
        CGFloat scale = MIN(viewSize.width / ciImageSize.width,
                            viewSize.height / ciImageSize.height);
        CGFloat offsetX = (viewSize.width - ciImageSize.width * scale) / 2;
        CGFloat offsetY = (viewSize.height - ciImageSize.height * scale) / 2;
        // ç¼©æ”¾
        CGAffineTransform scaleTransform = CGAffineTransformMakeScale(scale, scale);
        //è·å–äººè„¸çš„frame
        CGRect faceViewBounds = CGRectApplyAffineTransform(faceFeature.bounds, transform);
        // ä¿®æ­£
        faceViewBounds = CGRectApplyAffineTransform(faceViewBounds,scaleTransform);
        faceViewBounds.origin.x += offsetX;
        faceViewBounds.origin.y += offsetY;
        
        UIView *faceView=[[UIView alloc] initWithFrame:faceViewBounds];
        faceView.layer.borderWidth=3;
        faceView.layer.borderColor=[UIColor orangeColor].CGColor;
        
        [self.oldImageView addSubview:faceView];
        
        /** åŠ å…‰ç¯  **/
        UIImageView *imageView=[UIImageView new];
        CGFloat haloWidth= faceViewBounds.size.width;
        CGFloat haloHeight= haloWidth * 159 / 351;
        
        CGFloat haloCenterX=faceViewBounds.origin.x+faceViewBounds.size.width/2;
        
        CGRect rect=CGRectMake(haloCenterX-haloWidth/2, faceViewBounds.origin.y-haloHeight, haloWidth, haloHeight);
        imageView.frame=rect;
        [self.oldImageView addSubview:imageView];
        
        
        NSMutableArray *list=[NSMutableArray new];
        for (int i=0; i<41; i++) {
            if (i<10) {
                NSString *name=[NSString stringWithFormat:@"halo_00%d",i];
                UIImage  *image=  [UIImage imageNamed:name];
                [list addObject:image];
            }else{
                NSString *name=[NSString stringWithFormat:@"halo_0%d",i];
                UIImage  *image=  [UIImage imageNamed:name];
                [list addObject:image];
            }
        }
        
        [imageView playGifAnim:[list copy]];
        
        // åˆ¤æ–­æ˜¯å¦æœ‰å·¦çœ¼ä½ç½®
        if(faceFeature.hasLeftEyePosition){
            
            CGFloat x=faceFeature.leftEyePosition.x;
            CGFloat y=faceFeature.leftEyePosition.y;
            CGRect leftEyeRect=CGRectMake(x-10/2,y-10/2, 10, 10);
            
            //è·å–äººè„¸çš„frame
            CGRect leftEyeBounds = CGRectApplyAffineTransform(leftEyeRect, transform);
            leftEyeBounds=CGRectApplyAffineTransform(leftEyeBounds,scaleTransform);
            leftEyeBounds.origin.x += offsetX;
            leftEyeBounds.origin.y += offsetY;
            
            UIView *leftEyeView = [[UIView alloc] initWithFrame:leftEyeBounds];
            leftEyeView .backgroundColor = [UIColor orangeColor];
            [self.oldImageView addSubview:leftEyeView ];
            
        }
        // åˆ¤æ–­æ˜¯å¦æœ‰å³çœ¼ä½ç½®
        if(faceFeature.hasRightEyePosition){
            CGFloat x=faceFeature.rightEyePosition.x;
            CGFloat y=faceFeature.rightEyePosition.y;
            CGRect rightEyeRect=CGRectMake(x-10/2,y-10/2, 10, 10);
            
            //è·å–äººè„¸çš„frame
            CGRect rightEyeBounds = CGRectApplyAffineTransform(rightEyeRect, transform);
            rightEyeBounds=CGRectApplyAffineTransform(rightEyeBounds,scaleTransform);
            rightEyeBounds.origin.x += offsetX;
            rightEyeBounds.origin.y += offsetY;
            
            UIView *rightEyeView = [[UIView alloc] initWithFrame:rightEyeBounds];
            rightEyeView.backgroundColor = [UIColor orangeColor];
            [self.oldImageView addSubview:rightEyeView];
            
        }
        // åˆ¤æ–­æ˜¯å¦æœ‰å˜´ä½ç½®
        if(faceFeature.hasMouthPosition){
            
        }
        
    }
    
}
```

### åŠ¨æ€åˆæˆ

å¼•ç”¨å¤´æ–‡ä»¶ï¼š

```
#import <GPUImage/GPUImage.h>
```

æ ¸å¿ƒæ–¹æ³•ï¼š
```
_faceThinking = YES;
CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
CFDictionaryRef attachments = CMCopyDictionaryOfAttachments(kCFAllocatorDefault, sampleBuffer, kCMAttachmentMode_ShouldPropagate);
CIImage *convertedImage = [[CIImage alloc] initWithCVPixelBuffer:pixelBuffer options:(__bridge NSDictionary *)attachments];

if (attachments)
    CFRelease(attachments);
NSDictionary *imageOptions = nil;
UIDeviceOrientation curDeviceOrientation = [[UIDevice currentDevice] orientation];
int exifOrientation;

enum {
    PHOTOS_EXIF_0ROW_TOP_0COL_LEFT			= 1, //   1  =  0th row is at the top, and 0th column is on the left (THE DEFAULT).
    PHOTOS_EXIF_0ROW_TOP_0COL_RIGHT			= 2, //   2  =  0th row is at the top, and 0th column is on the right.
    PHOTOS_EXIF_0ROW_BOTTOM_0COL_RIGHT      = 3, //   3  =  0th row is at the bottom, and 0th column is on the right.
    PHOTOS_EXIF_0ROW_BOTTOM_0COL_LEFT       = 4, //   4  =  0th row is at the bottom, and 0th column is on the left.
    PHOTOS_EXIF_0ROW_LEFT_0COL_TOP          = 5, //   5  =  0th row is on the left, and 0th column is the top.
    PHOTOS_EXIF_0ROW_RIGHT_0COL_TOP         = 6, //   6  =  0th row is on the right, and 0th column is the top.
    PHOTOS_EXIF_0ROW_RIGHT_0COL_BOTTOM      = 7, //   7  =  0th row is on the right, and 0th column is the bottom.
    PHOTOS_EXIF_0ROW_LEFT_0COL_BOTTOM       = 8  //   8  =  0th row is on the left, and 0th column is the bottom.
};
BOOL isUsingFrontFacingCamera = FALSE;
AVCaptureDevicePosition currentCameraPosition = [self.videoCamera cameraPosition];

if (currentCameraPosition != AVCaptureDevicePositionBack) {
    isUsingFrontFacingCamera = TRUE;
}

switch (curDeviceOrientation) {
    case UIDeviceOrientationPortraitUpsideDown:  // Device oriented vertically, home button on the top
        exifOrientation = PHOTOS_EXIF_0ROW_LEFT_0COL_BOTTOM;
        break;
    case UIDeviceOrientationLandscapeLeft:       // Device oriented horizontally, home button on the right
        if (isUsingFrontFacingCamera)
            exifOrientation = PHOTOS_EXIF_0ROW_BOTTOM_0COL_RIGHT;
        else
            exifOrientation = PHOTOS_EXIF_0ROW_TOP_0COL_LEFT;
        break;
    case UIDeviceOrientationLandscapeRight:      // Device oriented horizontally, home button on the left
        if (isUsingFrontFacingCamera)
            exifOrientation = PHOTOS_EXIF_0ROW_TOP_0COL_LEFT;
        else
            exifOrientation = PHOTOS_EXIF_0ROW_BOTTOM_0COL_RIGHT;
        break;
    case UIDeviceOrientationPortrait:            // Device oriented vertically, home button on the bottom
    default:
        exifOrientation = PHOTOS_EXIF_0ROW_RIGHT_0COL_TOP;
        break;
}

imageOptions = [NSDictionary dictionaryWithObject:[NSNumber numberWithInt:exifOrientation] forKey:CIDetectorImageOrientation];
NSArray *features = [self.faceDetector featuresInImage:convertedImage options:imageOptions];

// get the clean aperture
CMFormatDescriptionRef fdesc = CMSampleBufferGetFormatDescription(sampleBuffer);
CGRect clap = CMVideoFormatDescriptionGetCleanAperture(fdesc, false /*originIsTopLeft == false*/);


[self GPUVCWillOutputFeatures:features forClap:clap andOrientation:curDeviceOrientation];
_faceThinking = NO;
```

ä½¿ç”¨ç®€å•ã€æ•ˆç‡é«˜æ•ˆã€è¿›ç¨‹å®‰å…¨~~~å¦‚æœä½ æœ‰æ›´å¥½çš„å»ºè®®,å¸Œæœ›ä¸åèµæ•™!

## âš– åè®®

```
MIT License

Copyright (c) 2017 ReverseScale

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## ğŸ˜¬  è”ç³»

* å¾®ä¿¡ : WhatsXie
* é‚®ä»¶ : ReverseScale@iCloud.com
* åšå®¢ : https://reversescale.github.io
